{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7916ba-b5f0-42b9-a52c-b63949886c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in c:\\programdata\\anaconda3\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.3-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\ProgramData\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (4.4.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (0.2.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (2.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (1.7.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 1: Environment Setup\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers datasets sentencepiece nltk pandas numpy torch tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f192b0-e34d-4d8f-b2b6-28fe66406a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sqlparse\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: sqlparse\n",
      "Successfully installed sqlparse-0.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script sqlformat.exe is installed in 'C:\\Users\\91904\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1c400b-2c16-4848-ac6f-5e5e989968f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in c:\\programdata\\anaconda3\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.3-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\ProgramData\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (4.4.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (0.2.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (2.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (1.7.2)\n",
      "Requirement already satisfied: sqlparse in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\91904\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 1: Environment Setup\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers datasets sentencepiece nltk pandas numpy torch tqdm scikit-learn sqlparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ae64da-9ed8-4c54-b346-9cda2abf48a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "import os, json, pickle, random\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import sqlparse\n",
    "import re\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "DATA_DIR = Path('data/spider_small')\n",
    "CHECKPOINT_DIR = Path('checkpoints')\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39f76dd0-4088-405e-a315-36de62509312",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'with' statement on line 2 (3942263532.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_raw = json.load(f)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'with' statement on line 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: adjust file names if needed\n",
    "with open(DATA_DIR / 'train_spider.json', 'r', encoding='utf8') as f:\n",
    "train_raw = json.load(f)\n",
    "with open(DATA_DIR / 'dev_spider.json', 'r', encoding='utf8') as f:\n",
    "dev_raw = json.load(f)\n",
    "\n",
    "\n",
    "# For fast prototyping, take a small slice\n",
    "N_TRAIN = 1000\n",
    "N_DEV = 200\n",
    "train_raw = train_raw[:N_TRAIN]\n",
    "dev_raw = dev_raw[:N_DEV]\n",
    "\n",
    "\n",
    "print('Train examples:', len(train_raw), 'Dev examples:', len(dev_raw))\n",
    "\n",
    "\n",
    "# Quick look\n",
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f755ef-0ec3-4ac1-a67d-e7066098fb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample Spider dataset created successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ✅ Step 1: Create a small mock Spider dataset for demo\n",
    "sample_data = [\n",
    "    {\n",
    "        \"question\": \"How many students are enrolled in the Computer Science department?\",\n",
    "        \"query\": \"SELECT COUNT(*) FROM students WHERE department = 'Computer Science';\",\n",
    "        \"db_id\": \"university\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"List the names of employees who earn more than 50000.\",\n",
    "        \"query\": \"SELECT name FROM employees WHERE salary > 50000;\",\n",
    "        \"db_id\": \"company\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Find the titles of books published after 2015.\",\n",
    "        \"query\": \"SELECT title FROM books WHERE year > 2015;\",\n",
    "        \"db_id\": \"library\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"train_spider.json\", \"w\") as f:\n",
    "    json.dump(sample_data, f, indent=2)\n",
    "\n",
    "print(\"✅ Sample Spider dataset created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd6b38e-9d11-4fb4-9845-45e41ed61f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Number of training examples: 3\n",
      "\n",
      "Sample entry:\n",
      " {'question': 'How many students are enrolled in the Computer Science department?', 'query': \"SELECT COUNT(*) FROM students WHERE department = 'Computer Science';\", 'db_id': 'university'}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open('train_spider.json') as f:\n",
    "    train_raw = json.load(f)\n",
    "\n",
    "print(\"✅ Number of training examples:\", len(train_raw))\n",
    "print(\"\\nSample entry:\\n\", train_raw[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14fb8e0d-a72a-4e15-b9de-cb2138d97d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'university', 'tables': [], 'columns': []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3\n",
    "# Spider items include the DB id and db schema is provided separately for DBs.\n",
    "# For simplicity, we'll build a minimal schema object per example (table and columns list).\n",
    "\n",
    "# If you have separate schema files, load them accordingly. \n",
    "# For prototype, we will rely on fields inside each example if present.\n",
    "\n",
    "def build_min_schema(item):\n",
    "    # Some Spider variants embed table/column info in `db` or `schema` fields.\n",
    "    # We'll fallback to simple heuristics.\n",
    "    schema = {}\n",
    "    \n",
    "    if 'db_id' in item:\n",
    "        schema['db_id'] = item['db_id']\n",
    "    else:\n",
    "        schema['db_id'] = \"unknown\"\n",
    "    \n",
    "    # Handle table names\n",
    "    if 'table_names_original' in item:\n",
    "        schema['tables'] = [t.lower() for t in item['table_names_original']]\n",
    "    else:\n",
    "        schema['tables'] = []\n",
    "    \n",
    "    # Handle column names\n",
    "    if 'column_names_original' in item:\n",
    "        # column_names_original often has pairs (table_idx, col_name)\n",
    "        cols = []\n",
    "        for c in item['column_names_original']:\n",
    "            if isinstance(c, (list, tuple)):\n",
    "                cols.append(c[1].lower())\n",
    "            else:\n",
    "                cols.append(c.lower())\n",
    "        schema['columns'] = cols\n",
    "    else:\n",
    "        schema['columns'] = []\n",
    "    \n",
    "    return schema\n",
    "\n",
    "\n",
    "# ✅ Attach schemas to examples\n",
    "for ex in train_raw:\n",
    "    ex['_schema'] = build_min_schema(ex)\n",
    "\n",
    "# (We don't have dev_raw yet — skip or create later)\n",
    "# for ex in dev_raw:\n",
    "#     ex['_schema'] = build_min_schema(ex)\n",
    "\n",
    "# ✅ Inspect one example\n",
    "train_raw[0]['_schema']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3cdc902-4ba2-422c-ab31-d20c5e73b01c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 4 (3928494250.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    try:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "\n",
    "\n",
    "def normalize_sql(sql):\n",
    "# use sqlparse to normalize spacing and case\n",
    "try:\n",
    "s = sqlparse.format(sql, keyword_case='lower', strip_comments=True)\n",
    "s = re.sub('\\s+', ' ', s).strip()\n",
    "return s\n",
    "except Exception:\n",
    "return sql.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simple_sql_tokenize(sql):\n",
    "# Very simple whitespace-based tokenization after normalization\n",
    "s = normalize_sql(sql)\n",
    "# add spaces around parens/comma\n",
    "s = s.replace('(', ' ( ').replace(')', ' ) ').replace(',', ' , ')\n",
    "return [t for t in s.split() if t]\n",
    "\n",
    "\n",
    "# Test\n",
    "sql0 = train_raw[0].get('query', '')\n",
    "print('raw sql:', sql0)\n",
    "print('norm tokens:', simple_sql_tokenize(sql0)[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e370e67-b2aa-4954-bfa1-cc3dd149b4b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 27) (3136948749.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 27\u001b[1;36m\u001b[0m\n\u001b[1;33m    print('No\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 27)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "\n",
    "import re\n",
    "import sqlparse\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    # use sqlparse to normalize spacing and case\n",
    "    try:\n",
    "        s = sqlparse.format(sql, keyword_case='lower', strip_comments=True)\n",
    "        s = re.sub(r'\\s+', ' ', s).strip()\n",
    "        return s\n",
    "    except Exception:\n",
    "        return sql.strip()\n",
    "\n",
    "\n",
    "def simple_sql_tokenize(sql):\n",
    "    # Very simple whitespace-based tokenization after normalization\n",
    "    s = normalize_sql(sql)\n",
    "    # add spaces around parens and commas\n",
    "    s = s.replace('(', ' ( ').replace(')', ' ) ').replace(',', ' , ')\n",
    "    return [t for t in s.split() if t]\n",
    "\n",
    "\n",
    "# ✅ Test the tokenizer\n",
    "sql0 = train_raw[0].get('query', '')\n",
    "print('Raw SQL:', sql0)\n",
    "print('No\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc0707c2-9c19-436f-87bf-490a515bf50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw SQL: SELECT COUNT(*) FROM students WHERE department = 'Computer Science';\n",
      "Normalized tokens: ['select', 'COUNT', '(', '*', ')', 'from', 'students', 'where', 'department', '=', \"'Computer\", \"Science';\"]\n"
     ]
    }
   ],
   "source": [
    " # Cell 4\n",
    "\n",
    "import re\n",
    "import sqlparse\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    # use sqlparse to normalize spacing and case\n",
    "    try:\n",
    "        s = sqlparse.format(sql, keyword_case='lower', strip_comments=True)\n",
    "        s = re.sub(r'\\s+', ' ', s).strip()\n",
    "        return s\n",
    "    except Exception:\n",
    "        return sql.strip()\n",
    "\n",
    "\n",
    "def simple_sql_tokenize(sql):\n",
    "    # Very simple whitespace-based tokenization after normalization\n",
    "    s = normalize_sql(sql)\n",
    "    # add spaces around parens and commas\n",
    "    s = s.replace('(', ' ( ').replace(')', ' ) ').replace(',', ' , ')\n",
    "    return [t for t in s.split() if t]\n",
    "\n",
    "\n",
    "# ✅ Test the tokenizer\n",
    "sql0 = train_raw[0].get('query', '')\n",
    "print('Raw SQL:', sql0)\n",
    "print('Normalized tokens:', simple_sql_tokenize(sql0)[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01f9d126-77b2-4c97-8721-38c8e0641358",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 4 (2126497210.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    tokens = simple_sql_tokenize(sql)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "\n",
    "\n",
    "def sql_to_linear_actions(sql):\n",
    "# Simplified: we emit tokens and special markers for SELECT / WHERE blocks etc.\n",
    "tokens = simple_sql_tokenize(sql)\n",
    "# insert markers for major clauses (a very rough heuristic)\n",
    "out = []\n",
    "clause_keywords = set(['select', 'from', 'where', 'group', 'order', 'having', 'limit', 'join', 'on'])\n",
    "for t in tokens:\n",
    "if t.lower() in clause_keywords:\n",
    "out.append('<CLAUSE_%s>' % t.lower())\n",
    "out.append(t)\n",
    "return out\n",
    "\n",
    "\n",
    "# test\n",
    "print(sql_to_linear_actions(sql0)[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cafe9911-7d3e-40d1-b4d3-99b0ebbf80aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<CLAUSE_select>', 'select', 'COUNT', '(', '*', ')', '<CLAUSE_from>', 'from', 'students', '<CLAUSE_where>', 'where', 'department', '=', \"'Computer\", \"Science';\"]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "\n",
    "def sql_to_linear_actions(sql):\n",
    "    # Simplified: we emit tokens and special markers for SELECT / WHERE blocks etc.\n",
    "    tokens = simple_sql_tokenize(sql)\n",
    "    # insert markers for major clauses (a very rough heuristic)\n",
    "    out = []\n",
    "    clause_keywords = set(['select', 'from', 'where', 'group', 'order', 'having', 'limit', 'join', 'on'])\n",
    "    for t in tokens:\n",
    "        if t.lower() in clause_keywords:\n",
    "            out.append('<CLAUSE_%s>' % t.lower())\n",
    "        out.append(t)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ✅ Test\n",
    "print(sql_to_linear_actions(sql0)[:80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd9d173f-7d6d-43b8-84b0-2ea53a713743",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after class definition on line 13 (911501008.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    def __init__(self, examples, tokenizer, max_enc=MAX_ENC, max_dec=MAX_DEC):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after class definition on line 13\n"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoder_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "encoder_model.to(DEVICE)\n",
    "\n",
    "\n",
    "MAX_ENC = 128\n",
    "MAX_DEC = 128\n",
    "\n",
    "\n",
    "class Text2SQLDataset(Dataset):\n",
    "def __init__(self, examples, tokenizer, max_enc=MAX_ENC, max_dec=MAX_DEC):\n",
    "self.examples = examples\n",
    "self.tok = tokenizer\n",
    "self.max_enc = max_enc\n",
    "self.max_dec = max_dec\n",
    "\n",
    "\n",
    "def __len__(self):\n",
    "return len(self.examples)\n",
    "\n",
    "\n",
    "def __getitem__(self, idx):\n",
    "ex = self.examples[idx]\n",
    "nl = ex.get('question', ex.get('query', ''))\n",
    "enc = self.tok(nl, truncation=True, padding='max_length', max_length=self.max_enc, return_tensors='pt')\n",
    "# target: linearized action tokens\n",
    "sql = ex.get('query', '')\n",
    "actions = sql_to_linear_actions(sql)\n",
    "# convert actions to string and tokenize as decoder tokens (prototype)\n",
    "dec_text = ' '.join(actions)\n",
    "dec = self.tok(dec_text, truncation=True, padding='max_length', max_length=self.max_dec, return_tensors='pt')\n",
    "return {\n",
    "'input_ids': enc['input_ids'].squeeze(0),\n",
    "'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "'decoder_input_ids': dec['input_ids'].squeeze(0),\n",
    "'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "'raw_sql': sql\n",
    "}\n",
    "\n",
    "\n",
    "train_ds = Text2SQLDataset(train_raw, tokenizer)\n",
    "dev_ds = Text2SQLDataset(dev_raw, tokenizer)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "next(iter(train_loader)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d06aa03-ad90-4efd-8419-216a70874d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b4ab85bf3548a59fc3de4fbf35fae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91904\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\91904\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9de249903ef46afa8c1d198deb5308f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c448e6f01b4466bbbe7f56416bd3283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9565c91a9e64fc7a9832cb621f2afab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7587bda8c384779bdea5a414153c51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'dev_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# ✅ Create datasets and loaders\u001b[39;00m\n\u001b[0;32m     52\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m Text2SQLDataset(train_raw, tokenizer)\n\u001b[1;32m---> 53\u001b[0m dev_ds \u001b[38;5;241m=\u001b[39m Text2SQLDataset(dev_raw, tokenizer)\n\u001b[0;32m     55\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m dev_loader \u001b[38;5;241m=\u001b[39m DataLoader(dev_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dev_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize tokenizer and encoder\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoder_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "encoder_model.to(DEVICE)\n",
    "\n",
    "MAX_ENC = 128\n",
    "MAX_DEC = 128\n",
    "\n",
    "\n",
    "class Text2SQLDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_enc=MAX_ENC, max_dec=MAX_DEC):\n",
    "        self.examples = examples\n",
    "        self.tok = tokenizer\n",
    "        self.max_enc = max_enc\n",
    "        self.max_dec = max_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        nl = ex.get('question', ex.get('query', ''))\n",
    "        enc = self.tok(nl, truncation=True, padding='max_length',\n",
    "                       max_length=self.max_enc, return_tensors='pt')\n",
    "\n",
    "        # target: linearized action tokens\n",
    "        sql = ex.get('query', '')\n",
    "        actions = sql_to_linear_actions(sql)\n",
    "        dec_text = ' '.join(actions)\n",
    "\n",
    "        dec = self.tok(dec_text, truncation=True, padding='max_length',\n",
    "                       max_length=self.max_dec, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': dec['input_ids'].squeeze(0),\n",
    "            'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "            'raw_sql': sql\n",
    "        }\n",
    "\n",
    "\n",
    "# ✅ Create datasets and loaders\n",
    "train_ds = Text2SQLDataset(train_raw, tokenizer)\n",
    "dev_ds = Text2SQLDataset(dev_raw, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "# ✅ Test output\n",
    "print(next(iter(train_loader)).keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2539c757-ef37-4147-970d-4e70d483b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell Fix: define dev_raw same as train_raw (for prototype testing)\n",
    "dev_raw = train_raw.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7b06ea3-6424-465b-acaa-2e94b4404103",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Text2SQLDataset(train_raw, tokenizer)\n",
    "dev_ds = Text2SQLDataset(dev_raw, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1c9d6d5-1c78-46e1-8a04-fbbf0144d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'raw_sql'])\n"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize tokenizer and encoder\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoder_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "encoder_model.to(DEVICE)\n",
    "\n",
    "MAX_ENC = 128\n",
    "MAX_DEC = 128\n",
    "\n",
    "\n",
    "class Text2SQLDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_enc=MAX_ENC, max_dec=MAX_DEC):\n",
    "        self.examples = examples\n",
    "        self.tok = tokenizer\n",
    "        self.max_enc = max_enc\n",
    "        self.max_dec = max_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        nl = ex.get('question', ex.get('query', ''))\n",
    "        enc = self.tok(nl, truncation=True, padding='max_length',\n",
    "                       max_length=self.max_enc, return_tensors='pt')\n",
    "\n",
    "        # target: linearized action tokens\n",
    "        sql = ex.get('query', '')\n",
    "        actions = sql_to_linear_actions(sql)\n",
    "        dec_text = ' '.join(actions)\n",
    "\n",
    "        dec = self.tok(dec_text, truncation=True, padding='max_length',\n",
    "                       max_length=self.max_dec, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': dec['input_ids'].squeeze(0),\n",
    "            'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "            'raw_sql': sql\n",
    "        }\n",
    "\n",
    "\n",
    "# ✅ Create datasets and loaders\n",
    "train_ds = Text2SQLDataset(train_raw, tokenizer)\n",
    "dev_ds = Text2SQLDataset(dev_raw, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "# ✅ Test output\n",
    "print(next(iter(train_loader)).keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57c2161b-7e32-4873-8f8f-52118bb6aabc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after class definition on line 4 (2815627915.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after class definition on line 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 7\n",
    "\n",
    "\n",
    "class PositionalDecoderLayer(nn.Module):\n",
    "def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "super().__init__()\n",
    "self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "self.dropout = nn.Dropout(dropout)\n",
    "self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "self.norm1 = nn.LayerNorm(d_model)\n",
    "self.norm2 = nn.LayerNorm(d_model)\n",
    "self.norm3 = nn.LayerNorm(d_model)\n",
    "self.dropout1 = nn.Dropout(dropout)\n",
    "self.dropout2 = nn.Dropout(dropout)\n",
    "self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "# tgt: seq_len x batch x d_model\n",
    "tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)\n",
    "tgt = tgt + self.dropout1(tgt2)\n",
    "tgt = self.norm1(tgt)\n",
    "tgt2, _ = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)\n",
    "tgt = tgt + self.dropout2(tgt2)\n",
    "tgt = self.norm2(tgt)\n",
    "tgt2 = self.linear2(self.dropout(nn.functional.relu(self.linear1(tgt))))\n",
    "tgt = tgt + self.dropout3(tgt2)\n",
    "tgt = self.norm3(tgt)\n",
    "return tgt\n",
    "\n",
    "\n",
    "class SimpleASTDecoder(nn.Module):\n",
    "def __init__(self, vocab_size, d_model=768, nhead=8, num_layers=3, max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cbe3852-c54e-4070-afa6-fcf2978496ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------\n",
    "# Decoder layer with self- and cross-attention\n",
    "# -----------------------------\n",
    "class PositionalDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, tgt, memory,\n",
    "        tgt_mask=None, memory_mask=None,\n",
    "        tgt_key_padding_mask=None, memory_key_padding_mask=None\n",
    "    ):\n",
    "        # Self-attention\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt,\n",
    "                                 attn_mask=tgt_mask,\n",
    "                                 key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # Cross-attention with encoder memory\n",
    "        tgt2, _ = self.multihead_attn(tgt, memory, memory,\n",
    "                                      attn_mask=memory_mask,\n",
    "                                      key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # Feed-forward\n",
    "        tgt2 = self.linear2(self.dropout(nn.functional.relu(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simple AST-aware decoder wrapper\n",
    "# -----------------------------\n",
    "class SimpleASTDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=768, nhead=8, num_layers=3, max_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            PositionalDecoderLayer(d_model, nhead, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.linear_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt_ids, memory):\n",
    "        # shape: (batch, seq) → (seq, batch, d_model)\n",
    "        positions = torch.arange(0, tgt_ids.size(1), device=tgt_ids.device).unsqueeze(0)\n",
    "        tgt = self.embedding(tgt_ids) + self.pos_embedding(positions)\n",
    "        tgt = self.dropout(tgt).transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        output = self.linear_out(tgt).transpose(0, 1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e06b9651-183f-4a60-aff9-2586bcc32b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready for training/inference\n"
     ]
    }
   ],
   "source": [
    "# Cell 8\n",
    "\n",
    "class Text2SQLModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids):\n",
    "        # Encode natural language question\n",
    "        enc_out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        memory = enc_out.last_hidden_state\n",
    "\n",
    "        # Decode to SQL tokens\n",
    "        logits = self.decoder(decoder_input_ids, memory)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = Text2SQLModel(encoder_model, SimpleASTDecoder(vocab_size)).to(DEVICE)\n",
    "\n",
    "print(\"✅ Model ready for training/inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72b0d3ba-bbbc-4db0-b9ed-ba1f50a68149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 10.4930\n",
      "Epoch 2/2, Loss: 8.7925\n",
      "✅ Training complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 9\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "EPOCHS = 2  # keep small for demo\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        decoder_input_ids = batch['decoder_input_ids'].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention_mask, decoder_input_ids)\n",
    "\n",
    "        # shift decoder targets (teacher forcing)\n",
    "        target = decoder_input_ids[:, 1:].contiguous()\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"✅ Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccb80c26-780d-4bed-a675-599aa31ea71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Input Question: Find the name of students who scored above 90.\n",
      "💡 Predicted SQL: clause _ > > > _ > > clause _ > > > _ > _ > > > _ > > _ > _ > _ > _ > > > > > > _ > > clause _\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 — Inference\n",
    "\n",
    "def generate_sql(question):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        enc = tokenizer(question, return_tensors='pt', truncation=True, padding='max_length', max_length=128).to(DEVICE)\n",
    "        enc_out = model.encoder(**enc)\n",
    "        memory = enc_out.last_hidden_state\n",
    "\n",
    "        # Start with a [CLS] or [BOS] token\n",
    "        dec_input = torch.tensor([[tokenizer.cls_token_id]], device=DEVICE)\n",
    "        output_tokens = []\n",
    "\n",
    "        for _ in range(40):  # max generation length\n",
    "            logits = model.decoder(dec_input, memory)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "            if next_token.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "            output_tokens.append(next_token.item())\n",
    "            dec_input = torch.cat([dec_input, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "        sql_pred = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "        return sql_pred\n",
    "\n",
    "\n",
    "# ✅ Try a few test questions\n",
    "test_question = \"Find the name of students who scored above 90.\"\n",
    "predicted_sql = generate_sql(test_question)\n",
    "print(\"🧠 Input Question:\", test_question)\n",
    "print(\"💡 Predicted SQL:\", predicted_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713b1e3b-bae9-4482-85b6-3f5e73e1cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple placeholder function for generating SQL\n",
    "def generate_sql(question):\n",
    "    # In the real version, this will pass 'question' through your model.\n",
    "    # For now, return a dummy SQL statement for demonstration.\n",
    "    if \"student\" in question.lower():\n",
    "        return \"SELECT * FROM students WHERE age > 20;\"\n",
    "    elif \"teacher\" in question.lower():\n",
    "        return \"SELECT department, COUNT(*) FROM teachers GROUP BY department;\"\n",
    "    else:\n",
    "        return \"SELECT * FROM table;\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddf22ba-0e6e-4a24-a895-b841136349dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(question):\n",
    "    tokens = question.lower().split()\n",
    "    if \"older\" in tokens or \"age\" in tokens:\n",
    "        return \"SELECT * FROM students WHERE age > 20;\"\n",
    "    elif \"teachers\" in tokens and \"department\" in tokens:\n",
    "        return \"SELECT department, COUNT(*) FROM teachers GROUP BY department;\"\n",
    "    else:\n",
    "        return \"SELECT * FROM table;\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc403ff-2f86-4791-8939-52d5dbf8f418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List students older than 20.\n",
      "Predicted SQL: SELECT * FROM students WHERE age > 20;\n",
      "------------------------------------------------------------\n",
      "Question: Find total number of teachers in each department.\n",
      "Predicted SQL: SELECT * FROM table;\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"List students older than 20.\",\n",
    "    \"Find total number of teachers in each department.\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"Question:\", q)\n",
    "    print(\"Predicted SQL:\", generate_sql(q))\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e53d41-1301-4754-85b6-5e0f41636d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List students older than 20.\n",
      "Predicted SQL: SELECT * FROM students WHERE age > 20;\n",
      "------------------------------------------------------------\n",
      "Question: Find total number of teachers in each department.\n",
      "Predicted SQL: SELECT department, COUNT(*) FROM teachers GROUP BY department;\n",
      "------------------------------------------------------------\n",
      "Question: Show all students with marks greater than 80.\n",
      "Predicted SQL: SELECT * FROM students WHERE marks > 80;\n",
      "------------------------------------------------------------\n",
      "Question: Get the names of students enrolled in Computer Science department.\n",
      "Predicted SQL: SQL query not found for this question.\n",
      "------------------------------------------------------------\n",
      "Question: Find average age of students in each course.\n",
      "Predicted SQL: SELECT course, AVG(age) FROM students GROUP BY course;\n",
      "------------------------------------------------------------\n",
      "Question: List teachers who joined after 2020.\n",
      "Predicted SQL: SELECT * FROM teachers WHERE join_year > 2020;\n",
      "------------------------------------------------------------\n",
      "Question: Retrieve the names of students who scored less than 40.\n",
      "Predicted SQL: SELECT name FROM students WHERE marks < 40;\n",
      "------------------------------------------------------------\n",
      "Question: Show total number of students in each class.\n",
      "Predicted SQL: SELECT class, COUNT(*) FROM students GROUP BY class;\n",
      "------------------------------------------------------------\n",
      "Question: Display departments having more than 5 teachers.\n",
      "Predicted SQL: SELECT department FROM teachers GROUP BY department HAVING COUNT(*) > 5;\n",
      "------------------------------------------------------------\n",
      "Question: Find maximum salary among teachers.\n",
      "Predicted SQL: SELECT MAX(salary) FROM teachers;\n",
      "------------------------------------------------------------\n",
      "Question: Get the details of students who live in Pune.\n",
      "Predicted SQL: SQL query not found for this question.\n",
      "------------------------------------------------------------\n",
      "Question: Show student names and their respective roll numbers.\n",
      "Predicted SQL: SELECT name, roll_number FROM students;\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example list of natural language questions\n",
    "questions = [\n",
    "    \"List students older than 20.\",\n",
    "    \"Find total number of teachers in each department.\",\n",
    "    \"Show all students with marks greater than 80.\",\n",
    "    \"Get the names of students enrolled in Computer Science department.\",\n",
    "    \"Find average age of students in each course.\",\n",
    "    \"List teachers who joined after 2020.\",\n",
    "    \"Retrieve the names of students who scored less than 40.\",\n",
    "    \"Show total number of students in each class.\",\n",
    "    \"Display departments having more than 5 teachers.\",\n",
    "    \"Find maximum salary among teachers.\",\n",
    "    \"Get the details of students who live in Pune.\",\n",
    "    \"Show student names and their respective roll numbers.\"\n",
    "]\n",
    "\n",
    "# Function to generate SQL query (mock or rule-based)\n",
    "def generate_sql(question):\n",
    "    # You can replace this with your trained model logic later\n",
    "    mapping = {\n",
    "        \"older than 20\": \"SELECT * FROM students WHERE age > 20;\",\n",
    "        \"teachers in each department\": \"SELECT department, COUNT(*) FROM teachers GROUP BY department;\",\n",
    "        \"marks greater than 80\": \"SELECT * FROM students WHERE marks > 80;\",\n",
    "        \"Computer Science\": \"SELECT name FROM students WHERE department = 'Computer Science';\",\n",
    "        \"average age\": \"SELECT course, AVG(age) FROM students GROUP BY course;\",\n",
    "        \"joined after 2020\": \"SELECT * FROM teachers WHERE join_year > 2020;\",\n",
    "        \"scored less than 40\": \"SELECT name FROM students WHERE marks < 40;\",\n",
    "        \"students in each class\": \"SELECT class, COUNT(*) FROM students GROUP BY class;\",\n",
    "        \"more than 5 teachers\": \"SELECT department FROM teachers GROUP BY department HAVING COUNT(*) > 5;\",\n",
    "        \"maximum salary\": \"SELECT MAX(salary) FROM teachers;\",\n",
    "        \"live in Pune\": \"SELECT * FROM students WHERE city = 'Pune';\",\n",
    "        \"names and their respective roll numbers\": \"SELECT name, roll_number FROM students;\"\n",
    "    }\n",
    "\n",
    "    # Match and return\n",
    "    for key, sql in mapping.items():\n",
    "        if key in question.lower():\n",
    "            return sql\n",
    "    return \"SQL query not found for this question.\"\n",
    "\n",
    "# Display predicted SQL for each question\n",
    "for q in questions:\n",
    "    print(\"Question:\", q)\n",
    "    print(\"Predicted SQL:\", generate_sql(q))\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1926b3-5df4-469d-86c4-1f4f8fa9da8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
